---
title: "Simple Linear Regression"
format:
  html:
    self-contained: true
    code-fold: true
warning: false
---



```{r}

library(tidyverse)
library(mosaic)
library(rio)
library(car)

math <- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')


```

# Review

Consider the relationship between Score on a math exam and a student's self-reported Confidence Rating.

What is the explanatory (aka independent) variable?

What is the response (aka the dependent) variable?  

Plot the relationship:

```{r}

#plot()

```

Does the relationship appear linear?

What is the direction of the relationship?

What do you think is the strength of the relationship?  (Strong/Moderate/Weak)

What is the correlation coefficient, r?

```{r}

#cor()

```


# Linear Equation

The correlation coefficient, r, tells us something about the direction and strength of a __linear__ relationship.

Linear Regression allows us to find a functional relationship between X and Y.  We can estimate the slope and intercept of the line that "best fits" the data and use that line to make predictions.  

Recall that when we had a quantitative response variable and a categorical explanatory variable we could use the `t.test()` if the categorical variable had 2 levels or the `aov()` function if the categorical variable had multiple levels.  We used the same notation for both: 
<br>
`t.test(y ~ x, data = dat)` and
`aov(y ~ x, data = dat)`
<br>

The `aov()` function output wasn't as useful as the `t.test()` by itself, so we typically assigned a name to the `aov()` output then used `summarize()` to get the test statistic and p-value.  We could also extract "residuals".

The same paradigm works for estimating the linear relationship between 2 quantitative variables, but we need to introduce a new function:  `lm()`.  The 'lm' stands for "linear model".  The relationship notation remains the same:
<br>

`lm(y ~ x, data = dat)`
<br>

except x and y are both quantitative.  Just like with `aov()` the `lm()` output by itself doesn't give us test statistics and p-values.  We will also use the `summarize()` function to get our test statistics.  

Let's fit a linear model estimating the slope and intercept of the line that best fits the relationship between Confidence Rating and Test Score:

```{r}

#math_lm <- lm()
#math_lm

```

The `lm()` output includes only the slope and the intercept. 

How do we interpret the intercept of 18.69?

How do we interpret the slope of 12.69 in context? 

What is the expected score on a test of someone who ranks themselves 5 in confidence? 


## Plotting the Regression Line

Scatter plots by themselves are nice, but we would also like to see the regression line.  Simple graphics in R can be augmented by using some functions.  The `abline()` function, when executed right after a graphing function can add lines.  We've used this to add vertical lines and horizontal line already in class.  We can also use this function to add a regression line.  We simply insert our linear model output into the `abline()` function as follows:

```{r}

#plot()
#abline()

```

Just as with the other plotting functions we've used, we can change the color, type and width of the line:

```{r}

#plot() # pch stands for "plot character" try a few
#abline() # lwd stands for line width, lty for line type


```


## Hypothesis Testing for Regression

A linear equation has 2 parameters: Slope and Intercept.  In most situations, the intercept isn't very interesting by itself and is often absurd. We are most interested in the slope

$$H_o: \beta_1 = ??$$
$$H_a: \beta_1 $$

These are the same for all regression questions.  

To get the p-value and test statistics, we use the `summary()` function as we did with aov:

```{r}

#summary(math_lm)

```


We can also calculate confidence intervals for the slope by using the `confint()` function.  This function requires you to tell it which model to extract a confidence intervals from.  You can specify which parameter you're interested in, and the level of confidence:  

```{r}

# input the model into the following function:
#confint()


```


How do we interpret this confidence interval for a slope?

95% Confident that...

For every 1 unit increase in Confidence Rating, test scores go up by...on average.  


## Regression Requirements

There are certain requirements for all statistical tests to be valid.  For means, we needed to make sure that the Central Limit Theorem applied.  This meant that we had a large enough sample size (N>30) or that the population itself was normally distributed.  

For ANOVA, we had to check that the residuals were normally distributed and that the population standard deviations were the same.

Regression analysis has 5 requirements to be valid.  While this sounds daunting, in practice we can check most of them very quickly. 

1. Relationship between X and Y is Linear
2. The residuals, $\epsilon$, are normally distributed
3. The Variance of the error terms is constant for all values of X
4. The X’s are fixed and measured without error (i.e. X’s can be considered as known constants)
5. The observations are independent

The linear relationship is assessed visually with the scatter plot.  If there is obvious curvature or non-linearity then fitting a line isn't the best model.

We check the normality of the residuals with a qqplot() exactly as with the `aov()` output.

The constant Variance is checked with a new plot, that looks at how the predicted values relate to the residuals.  This is important because we want our predictions to be "wrong" about the same regardless of the value of the prediction.  We're looking for random scatter.

Requirements 4 cannot be analyzed directly.  It is important because X is the independent variable.  If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.

Requirement 5 also cannot be analyzed, but random sampling is usually satisfies this requirement.  

```{r}

# Requirement 1:  Check for linear relationship
#plot()

# Req 2: Normality of residuals:
#qqPlot()

# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2
#plot()



```


